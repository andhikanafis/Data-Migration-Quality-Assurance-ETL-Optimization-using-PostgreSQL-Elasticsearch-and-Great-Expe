
# Employee Attrition Data Analysis: Automated ETL from PostgreSQL to Elasticsearch Using Airflow & Great Expectations

## Problem Statement
This project aims to streamline and enhance the ETL processes in migrating data from a PostgreSQL database to Elasticsearch. While automation is a core objective, ensuring data quality, consistency, and reliability during these operations is equally crucial. Leveraging the capabilities of "Great Expectations," this project places a significant emphasis on validating data at each step, ensuring that the migrated data meets the defined quality standards. The dataset used in this initiative provides insights into factors influencing employee attrition rates in companies, making the need for precise and trustworthy data even more imperative.

## Objective
The main objectives are:
- Streamline the ETL process by automating data transformations and migrations between PostgreSQL and Elasticsearch.
- Ensure data quality and validation using Great Expectations.
- Provide insights into employee behavior and factors influencing attrition.

## Background
The project addresses the need for reliable and automated data processing to understand employee attrition rates. Accurate data is imperative for deriving meaningful insights into the factors influencing company employee turnover.

## Project Output
The project results in an automated, reliable ETL pipeline that ensures data quality from PostgreSQL to Elasticsearch. This pipeline facilitates the analysis of employee attrition factors with higher accuracy and efficiency.

## Data Description
The dataset encompasses various factors related to employee attrition rates, providing a comprehensive view of the elements influencing employee turnover in organizations.

## Methodology
- Automated ETL processes using Airflow.
- Data quality checks and validation with Great Expectations.
- Data migration from PostgreSQL to Elasticsearch.

## Technology Stack
- PostgreSQL
- Elasticsearch
- Apache Airflow
- Great Expectations for data validation
  
## How to Use
1. Ensure you have the required libraries and systems set up, including PostgreSQL, Elasticsearch, and Airflow.
2. Clone the repository containing the provided scripts and notebook.
3. Update the connection strings and configurations in the scripts to match your server details.
4. Execute the scripts to perform the ETL operations.
5. Open the Jupyter Notebook to perform data validation and quality checks using Great Expectations.

---

For any queries or feedback, please contact the author [Andhika Abdurachim Nafis](https://github.com/andhikanafis).
